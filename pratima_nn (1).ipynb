{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset we use here is 'mnist.pkl.gz' which is divided into training, validation and test data. The following function <i> load_data() </i> unpacks the file and extracts the training, validation and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "    f.seek(0)\n",
    "    training_data, validation_data, test_data = pickle.load(f, encoding='latin1')\n",
    "    f.close()\n",
    "    return (training_data, validation_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the data looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, validation_data, test_data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        ..., \n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32),\n",
       " array([5, 0, 4, ..., 8, 4, 8]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n",
      "(50000,)\n"
     ]
    }
   ],
   "source": [
    "# shape of data\n",
    "print(training_data[0].shape)\n",
    "print(training_data[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The feature dataset is:[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "The target dataset is:[5 0 4 ..., 8 4 8]\n",
      "The number of examples in the training dataset is:50000\n",
      "The number of points in a single input is:784\n"
     ]
    }
   ],
   "source": [
    "print(\"The feature dataset is:\" + str(training_data[0]))\n",
    "print(\"The target dataset is:\" + str(training_data[1]))\n",
    "print(\"The number of examples in the training dataset is:\" + str(len(training_data[0])))\n",
    "print(\"The number of points in a single input is:\" + str(len(training_data[0][1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as discussed earlier in the lectures, the target variable is converted to a one hot matrix. We use the function <i> one_hot </i> to convert the target dataset to one hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(j):\n",
    "    # input is the target dataset of shape (m,) where m is the number of data points\n",
    "    # returns a 2 dimensional array of shape (10, m) where each target value is converted to a one hot encoding\n",
    "    # Look at the next block of code for a better understanding of one hot encoding\n",
    "    n = j.shape[0]\n",
    "    new_array = np.zeros((10, n))\n",
    "    index = 0\n",
    "    for res in j:\n",
    "        new_array[res][index] = 1.0\n",
    "        index = index + 1\n",
    "    return new_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "print(data.shape)\n",
    "one_hot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function data_wrapper() will convert the dataset into the desired shape and also convert the ground truth labels to one_hot matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_wrapper():\n",
    "    tr_d, va_d, te_d = load_data()\n",
    "    \n",
    "    training_inputs = np.array(tr_d[0][:]).T\n",
    "    training_results = np.array(tr_d[1][:])\n",
    "    train_set_y = one_hot(training_results)\n",
    "    \n",
    "    validation_inputs = np.array(va_d[0][:]).T\n",
    "    validation_results = np.array(va_d[1][:])\n",
    "    validation_set_y = one_hot(validation_results)\n",
    "    \n",
    "    test_inputs = np.array(te_d[0][:]).T\n",
    "    test_results = np.array(te_d[1][:])\n",
    "    test_set_y = one_hot(test_results)\n",
    "    \n",
    "    return (training_inputs, train_set_y, test_inputs, test_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x, train_set_y, test_set_x, test_set_y = data_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_x shape: (784, 50000)\n",
      "train_set_y shape: (10, 50000)\n",
      "test_set_x shape: (784, 10000)\n",
      "test_set_y shape: (10, 10000)\n"
     ]
    }
   ],
   "source": [
    "print (\"train_set_x shape: \" + str(train_set_x.shape))\n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print (\"test_set_x shape: \" + str(test_set_x.shape))\n",
    "print (\"test_set_y shape: \" + str(test_set_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the data_wrapper has converted the training and validation data into numpy array of desired shapes. Let's convert the actual labels into a dataframe to see if the one hot conversions are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.DataFrame(train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The target dataset is:[5 0 4 ..., 8 4 8]\n",
      "The one hot encoding dataset is:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>49990</th>\n",
       "      <th>49991</th>\n",
       "      <th>49992</th>\n",
       "      <th>49993</th>\n",
       "      <th>49994</th>\n",
       "      <th>49995</th>\n",
       "      <th>49996</th>\n",
       "      <th>49997</th>\n",
       "      <th>49998</th>\n",
       "      <th>49999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 50000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0      1      2      3      4      5      6      7      8      9      \\\n",
       "0    0.0    1.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "1    0.0    0.0    0.0    1.0    0.0    0.0    1.0    0.0    1.0    0.0   \n",
       "2    0.0    0.0    0.0    0.0    0.0    1.0    0.0    0.0    0.0    0.0   \n",
       "3    0.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0    0.0    0.0   \n",
       "4    0.0    0.0    1.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0   \n",
       "5    1.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "6    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "7    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "8    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "9    0.0    0.0    0.0    0.0    1.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "   ...    49990  49991  49992  49993  49994  49995  49996  49997  49998  49999  \n",
       "0  ...      0.0    0.0    0.0    0.0    0.0    0.0    1.0    0.0    0.0    0.0  \n",
       "1  ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "2  ...      0.0    0.0    0.0    1.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "3  ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "4  ...      0.0    0.0    0.0    0.0    1.0    0.0    0.0    0.0    1.0    0.0  \n",
       "5  ...      0.0    1.0    1.0    0.0    0.0    1.0    0.0    0.0    0.0    0.0  \n",
       "6  ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "7  ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "8  ...      1.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0    0.0    1.0  \n",
       "9  ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "\n",
       "[10 rows x 50000 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The target dataset is:\" + str(training_data[1]))\n",
    "print(\"The one hot encoding dataset is:\")\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us visualise the dataset. Feel free to change the index to see if the training data has been correctly tagged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fafa2cae0f0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fafa2cae588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index  = 1000\n",
    "k = train_set_x[:,index]\n",
    "k = k.reshape((28, 28))\n",
    "plt.title('Label is {label}'.format(label= training_data[1][index]))\n",
    "plt.imshow(k, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sigmoid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \n",
    "    # Z is numpy array of shape (n, m) where n is number of neurons in the layer and m is the number of samples \n",
    "    # sigmoid_memory is stored as it is used later on in backpropagation\n",
    "    \n",
    "    H = 1/(1+np.exp(-Z))\n",
    "    sigmoid_memory = Z\n",
    "    \n",
    "    return H, sigmoid_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid(Z) = (array([[ 0.5       ,  0.73105858],\n",
      "       [ 0.88079708,  0.95257413],\n",
      "       [ 0.98201379,  0.99330715],\n",
      "       [ 0.99752738,  0.99908895]]), array([[0, 1],\n",
      "       [2, 3],\n",
      "       [4, 5],\n",
      "       [6, 7]]))\n"
     ]
    }
   ],
   "source": [
    "Z = np.arange(8).reshape(4,2)\n",
    "print (\"sigmoid(Z) = \" + str(sigmoid(Z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### relu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    # Z is numpy array of shape (n, m) where n is number of neurons in the layer and m is the number of samples \n",
    "    # relu_memory is stored as it is used later on in backpropagation\n",
    "    \n",
    "    H = np.maximum(0,Z)\n",
    "    \n",
    "    assert(H.shape == Z.shape)\n",
    "    \n",
    "    relu_memory = Z \n",
    "    return H, relu_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relu(Z) = (array([[ 1,  3],\n",
      "       [ 0,  0],\n",
      "       [ 0,  7],\n",
      "       [ 9, 18]]), array([[ 1,  3],\n",
      "       [-1, -4],\n",
      "       [-5,  7],\n",
      "       [ 9, 18]]))\n"
     ]
    }
   ],
   "source": [
    "Z = np.array([1, 3, -1, -4, -5, 7, 9, 18]).reshape(4,2)\n",
    "print (\"relu(Z) = \" + str(relu(Z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### softmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    # Z is numpy array of shape (n, m) where n is number of neurons in the layer and m is the number of samples \n",
    "    # softmax_memory is stored as it is used later on in backpropagation\n",
    "   \n",
    "    Z_exp = np.exp(Z)\n",
    "\n",
    "    Z_sum = np.sum(Z_exp,axis = 0, keepdims = True)\n",
    "    \n",
    "    H = Z_exp/Z_sum  #normalising step\n",
    "    softmax_memory = Z\n",
    "    \n",
    "    return H, softmax_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.array([[11,19,10], [12, 21, 23]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2.68941421e-01   1.19202922e-01   2.26032430e-06]\n",
      " [  7.31058579e-01   8.80797078e-01   9.99997740e-01]]\n",
      "[[11 19 10]\n",
      " [12 21 23]]\n"
     ]
    }
   ],
   "source": [
    "#Z = np.array(np.arange(30)).reshape(10,3)\n",
    "H, softmax_memory = softmax(Z)\n",
    "print(H)\n",
    "print(softmax_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialize_parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(dimensions):\n",
    "\n",
    "    # dimensions is a list containing the number of neuron in each layer in the network\n",
    "    # It returns parameters which is a python dictionary containing the parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "\n",
    "    np.random.seed(2)\n",
    "    parameters = {}\n",
    "    L = len(dimensions)            # number of layers in the network + 1\n",
    "\n",
    "    for l in range(1, L): \n",
    "        parameters['W' + str(l)] = np.random.randn(dimensions[l], dimensions[l-1]) * 0.1\n",
    "        parameters['b' + str(l)] = np.zeros((dimensions[l], 1)) \n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (dimensions[l], dimensions[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (dimensions[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-0.04167578 -0.00562668 -0.21361961 ..., -0.06168445  0.03213358\n",
      "  -0.09464469]\n",
      " [-0.05301394 -0.1259207   0.16775441 ..., -0.03284246 -0.05623108\n",
      "   0.01179136]\n",
      " [ 0.07386378 -0.15872956  0.01532001 ..., -0.08428557  0.10040469\n",
      "   0.00545832]]\n",
      "b1 = [[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "W2 = [[ 0.06650944 -0.19626047  0.2112715 ]\n",
      " [-0.28074571 -0.13967752  0.02641189]\n",
      " [ 0.10925169  0.06646016  0.08565535]\n",
      " [-0.11058228  0.03715795  0.13440124]\n",
      " [-0.16421272 -0.1153127   0.02013163]\n",
      " [ 0.13985659  0.07228733 -0.10717236]\n",
      " [-0.05673344 -0.03663499 -0.15460347]]\n",
      "b2 = [[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n"
     ]
    }
   ],
   "source": [
    "dimensions  = [784, 3,7,10]\n",
    "parameters = initialize_parameters(dimensions)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))\n",
    "# print(\"W3 = \" + str(parameters[\"W3\"]))\n",
    "# print(\"b3 = \" + str(parameters[\"b3\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### layer_forward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graded\n",
    "\n",
    "def layer_forward(H_prev, W, b, activation = 'relu'):\n",
    "\n",
    "    # H_prev is of shape (size of previous layer, number of examples)\n",
    "    # W is weights matrix of shape (size of current layer, size of previous layer)\n",
    "    # b is bias vector of shape (size of the current layer, 1)\n",
    "    # activation is the activation to be used for forward propagation : \"softmax\", \"relu\", \"sigmoid\"\n",
    "\n",
    "    # H is the output of the activation function \n",
    "    # memory is a python dictionary containing \"linear_memory\" and \"activation_memory\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        Z = np.dot(W, H_prev)+b\n",
    "        linear_memory = (H_prev, W, b)\n",
    "        H, activation_memory = sigmoid(Z) #write your code here\n",
    " \n",
    "    elif activation == \"softmax\":\n",
    "        Z = np.dot(W, H_prev)+b#write your code here\n",
    "        linear_memory = (H_prev, W, b)\n",
    "        H, activation_memory = softmax(Z) #write your code here\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        Z = np.dot(W, H_prev)+b#write your code here\n",
    "        linear_memory = (H_prev, W, b)\n",
    "        H, activation_memory = relu(Z) #write your code here\n",
    "        \n",
    "    assert (H.shape == (W.shape[0], H_prev.shape[1]))\n",
    "    memory = (linear_memory, activation_memory)\n",
    "\n",
    "    return H, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  1.        ,  1.        ,  1.        ,  1.        ],\n",
       "       [ 0.99908895,  0.99330715,  0.99999969,  1.        ,  0.99987661],\n",
       "       [ 0.73105858,  0.5       ,  0.99330715,  0.9999546 ,  0.88079708]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify\n",
    "# l-1 has two neurons, l has three, m = 5\n",
    "# H_prev is (l-1, m)\n",
    "# W is (l, l-1)\n",
    "# b is (l, 1)\n",
    "# H should be (l, m)\n",
    "H_prev = np.array([[1,0, 5, 10, 2], [2, 5, 3, 10, 2]])\n",
    "W_sample = np.array([[10, 5], [2, 0], [1, 0]])\n",
    "b_sample = np.array([10, 5, 0]).reshape((3, 1))\n",
    "\n",
    "#print(np.dot(W_sample, H_prev)+b_sample)\n",
    "#print(sigmoid(np.dot(W_sample, H_prev)+b_sample))\n",
    "#print(relu(np.dot(W_sample, H_prev)+b_sample))\n",
    "H = layer_forward(H_prev, W_sample, b_sample, activation=\"sigmoid\")[0]\n",
    "H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get:<br>\n",
    "    array([[1.        , 1.        , 1.        , 1.        , 1.        ],<br>\n",
    "      [0.99908895, 0.99330715, 0.99999969, 1.        , 0.99987661],<br>\n",
    "       [0.73105858, 0.5       , 0.99330715, 0.9999546 , 0.88079708]])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L_layer_forward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graded\n",
    "\n",
    "def L_layer_forward(X, parameters):\n",
    "\n",
    "    # X is input data of shape (input size, number of examples)\n",
    "    # parameters is output of initialize_parameters()\n",
    "    \n",
    "    # HL is the last layer's post-activation value\n",
    "    # memories is the list of memory containing (for a relu activation, for example):\n",
    "    # - every memory of relu forward (there are L-1 of them, indexed from 1 to L-1), \n",
    "    # - the memory of softmax forward (there is one, indexed L) \n",
    "\n",
    "    memories = []\n",
    "    H = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement relu layer (L-1) times as the Lth layer is the softmax layer\n",
    "    for l in range(1, L):\n",
    "        H_prev = H\n",
    "        W_l = parameters['W'+str(l)]\n",
    "        b_l = parameters['b'+str(l)]\n",
    "        H, memory = layer_forward(H_prev, W_l , b_l , activation = 'relu') #write your code here\n",
    "        memories.append(memory)\n",
    "    \n",
    "    # Implement the final softmax layer\n",
    "    # HL here is the final prediction P as specified in the lectures\n",
    "    W_L = parameters['W'+str(L)]\n",
    "    b_L = parameters['b'+str(L)]\n",
    "    HL, memory = layer_forward(H, W_L, b_L, activation = 'softmax') #write your code here\n",
    "    \n",
    "    memories.append(memory)\n",
    "\n",
    "    assert(HL.shape == (10, X.shape[1]))\n",
    "            \n",
    "    return HL, memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 10)\n",
      "[[ 0.10106734  0.10045152  0.09927757  0.10216656  0.1       ]\n",
      " [ 0.10567625  0.10230873  0.10170271  0.11250099  0.1       ]\n",
      " [ 0.09824287  0.0992886   0.09967128  0.09609693  0.1       ]\n",
      " [ 0.10028288  0.10013048  0.09998149  0.10046076  0.1       ]\n",
      " [ 0.09883601  0.09953443  0.09931419  0.097355    0.1       ]\n",
      " [ 0.10668575  0.10270912  0.10180736  0.11483609  0.1       ]\n",
      " [ 0.09832513  0.09932275  0.09954792  0.09627089  0.1       ]\n",
      " [ 0.09747092  0.09896735  0.0995387   0.09447277  0.1       ]\n",
      " [ 0.09489069  0.09788255  0.09929998  0.08915178  0.1       ]\n",
      " [ 0.09852217  0.09940447  0.09985881  0.09668824  0.1       ]]\n"
     ]
    }
   ],
   "source": [
    "# verify\n",
    "# X is (784, 10)\n",
    "# parameters is a dict\n",
    "# HL should be (10, 10)\n",
    "x_sample = train_set_x[:, 10:20]\n",
    "print(x_sample.shape)\n",
    "HL = L_layer_forward(x_sample, parameters=parameters)[0]\n",
    "print(HL[:, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get:\n",
    "\n",
    "(784, 10)<br>\n",
    "[[0.10106734 0.10045152 0.09927757 0.10216656 0.1       ]<br>\n",
    " [0.10567625 0.10230873 0.10170271 0.11250099 0.1       ]<br>\n",
    " [0.09824287 0.0992886  0.09967128 0.09609693 0.1       ]<br>\n",
    " [0.10028288 0.10013048 0.09998149 0.10046076 0.1       ]<br>\n",
    " [0.09883601 0.09953443 0.09931419 0.097355   0.1       ]<br>\n",
    " [0.10668575 0.10270912 0.10180736 0.11483609 0.1       ]<br>\n",
    " [0.09832513 0.09932275 0.09954792 0.09627089 0.1       ]<br>\n",
    " [0.09747092 0.09896735 0.0995387  0.09447277 0.1       ]<br>\n",
    " [0.09489069 0.09788255 0.09929998 0.08915178 0.1       ]<br>\n",
    " [0.09852217 0.09940447 0.09985881 0.09668824 0.1       ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss\n",
    "\n",
    "### compute_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graded\n",
    "\n",
    "def compute_loss(HL, Y):\n",
    "\n",
    "\n",
    "    # HL is probability matrix of shape (10, number of examples)\n",
    "    # Y is true \"label\" vector shape (10, number of examples)\n",
    "\n",
    "    # loss is the cross-entropy loss\n",
    "\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    loss =  -(1.0/m) * np.sum(Y*np.log(HL))  #write your code here, use (1./m) and not (1/m)\n",
    "    \n",
    "    loss = np.squeeze(loss)      # To make sure that the loss's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(loss.shape == ())\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.4359949   0.02592623  0.54966248  0.43532239  0.4203678 ]\n",
      " [ 0.33033482  0.20464863  0.61927097  0.29965467  0.26682728]\n",
      " [ 0.62113383  0.52914209  0.13457995  0.51357812  0.18443987]\n",
      " [ 0.78533515  0.85397529  0.49423684  0.84656149  0.07964548]\n",
      " [ 0.50524609  0.0652865   0.42812233  0.09653092  0.12715997]\n",
      " [ 0.59674531  0.226012    0.10694568  0.22030621  0.34982629]\n",
      " [ 0.46778748  0.20174323  0.64040673  0.48306984  0.50523672]\n",
      " [ 0.38689265  0.79363745  0.58000418  0.1622986   0.70075235]\n",
      " [ 0.96455108  0.50000836  0.88952006  0.34161365  0.56714413]\n",
      " [ 0.42754596  0.43674726  0.77655918  0.53560417  0.95374223]]\n",
      "[[ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "0.896460026133\n"
     ]
    }
   ],
   "source": [
    "# sample\n",
    "# HL is (10, 5), Y is (10, 5)\n",
    "np.random.seed(2)\n",
    "HL_sample = np.random.rand(10,5)\n",
    "Y_sample = train_set_y[:, 10:15]\n",
    "print(HL_sample)\n",
    "print(Y_sample)\n",
    "\n",
    "print(compute_loss(HL_sample, Y_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get:<br>\n",
    "    \n",
    "[[0.4359949  0.02592623 0.54966248 0.43532239 0.4203678 ]<br>\n",
    " [0.33033482 0.20464863 0.61927097 0.29965467 0.26682728]<br>\n",
    " [0.62113383 0.52914209 0.13457995 0.51357812 0.18443987]<br>\n",
    " [0.78533515 0.85397529 0.49423684 0.84656149 0.07964548]<br>\n",
    " [0.50524609 0.0652865  0.42812233 0.09653092 0.12715997]<br>\n",
    " [0.59674531 0.226012   0.10694568 0.22030621 0.34982629]<br>\n",
    " [0.46778748 0.20174323 0.64040673 0.48306984 0.50523672]<br>\n",
    " [0.38689265 0.79363745 0.58000418 0.1622986  0.70075235]<br>\n",
    " [0.96455108 0.50000836 0.88952006 0.34161365 0.56714413]<br>\n",
    " [0.42754596 0.43674726 0.77655918 0.53560417 0.95374223]]<br>\n",
    "[[0. 0. 0. 0. 0.]<br>\n",
    " [0. 0. 0. 0. 1.]<br>\n",
    " [0. 0. 0. 0. 0.]<br>\n",
    " [1. 0. 1. 0. 0.]<br>\n",
    " [0. 0. 0. 0. 0.]<br>\n",
    " [0. 1. 0. 0. 0.]<br>\n",
    " [0. 0. 0. 1. 0.]<br>\n",
    " [0. 0. 0. 0. 0.]<br>\n",
    " [0. 0. 0. 0. 0.]<br>\n",
    " [0. 0. 0. 0. 0.]]<br>\n",
    "0.8964600261334037"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "\n",
    "### sigmoid-backward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dH, sigmoid_memory):\n",
    "    \n",
    "    # Implement the backpropagation of a sigmoid function\n",
    "    # dH is gradient of the sigmoid activated activation of shape same as H or Z in the same layer    \n",
    "    # sigmoid_memory is the memory stored in the sigmoid(Z) calculation\n",
    "    \n",
    "    Z = sigmoid_memory\n",
    "    \n",
    "    H = 1/(1+np.exp(-Z))\n",
    "    dZ = dH * H * (1-H)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### relu-backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dH, relu_memory):\n",
    "    \n",
    "    # Implement the backpropagation of a relu function\n",
    "    # dH is gradient of the relu activated activation of shape same as H or Z in the same layer    \n",
    "    # relu_memory is the memory stored in the sigmoid(Z) calculation\n",
    "    \n",
    "    Z = relu_memory\n",
    "    dZ = np.array(dH, copy=True) # dZ will be the same as dA wherever the elements of A weren't 0\n",
    "    \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### layer_backward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graded\n",
    "\n",
    "def layer_backward(dH, memory, activation = 'relu'):\n",
    "    \n",
    "    # takes dH and the memory calculated in layer_forward and activation as input to calculate the dH_prev, dW, db\n",
    "    # performs the backprop depending upon the activation function\n",
    "    \n",
    "    linear_memory, activation_memory = memory\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dH, activation_memory)\n",
    "        H_prev, W, b = linear_memory\n",
    "        m = H_prev.shape[1]\n",
    "        dW = (1.0/m) * np.matmul(dZ, H_prev.T)\n",
    "        db = (1.0/m) * np.sum(dZ, axis=-1, keepdims=True)\n",
    "        dH_prev = np.matmul(np.transpose(W), dZ)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dH, activation_memory) #write your code here\n",
    "        H_prev, W, b = linear_memory\n",
    "        m = H_prev.shape[1]\n",
    "        dW = (1.0/m) * np.matmul(dZ, H_prev.T)\n",
    "        db = (1.0/m) * np.sum(dZ, axis=-1, keepdims=True)\n",
    "        dH_prev = np.matmul(np.transpose(W), dZ)\n",
    "    \n",
    "    return dH_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dH_prev is \n",
      " [[ 5.6417525   0.66855959  6.86974666  5.46611139  4.92177244]\n",
      " [ 2.17997451  0.12963116  2.74831239  2.17661196  2.10183901]]\n",
      "dW is \n",
      " [[ 1.67565336  1.56891359]\n",
      " [ 1.39137819  1.4143854 ]\n",
      " [ 1.3597389   1.43013369]]\n",
      "db is \n",
      " [[ 0.37345476]\n",
      " [ 0.34414727]\n",
      " [ 0.29074635]]\n"
     ]
    }
   ],
   "source": [
    "# verify\n",
    "# l-1 has two neurons, l has three, m = 5\n",
    "# H_prev is (l-1, m)\n",
    "# W is (l, l-1)\n",
    "# b is (l, 1)\n",
    "# H should be (l, m)\n",
    "H_prev = np.array([[1,0, 5, 10, 2], [2, 5, 3, 10, 2]])\n",
    "W_sample = np.array([[10, 5], [2, 0], [1, 0]])\n",
    "b_sample = np.array([10, 5, 0]).reshape((3, 1))\n",
    "\n",
    "H, memory = layer_forward(H_prev, W_sample, b_sample, activation=\"relu\")\n",
    "np.random.seed(2)\n",
    "dH = np.random.rand(3,5)\n",
    "dH_prev, dW, db = layer_backward(dH, memory, activation = 'relu')\n",
    "print('dH_prev is \\n' , dH_prev)\n",
    "print('dW is \\n' ,dW)\n",
    "print('db is \\n', db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get:<br>\n",
    "dH_prev is <br>\n",
    " [[5.6417525  0.66855959 6.86974666 5.46611139 4.92177244]<br>\n",
    " [2.17997451 0.12963116 2.74831239 2.17661196 2.10183901]]<br>\n",
    "dW is <br>\n",
    " [[1.67565336 1.56891359]<br>\n",
    " [1.39137819 1.4143854 ]<br>\n",
    " [1.3597389  1.43013369]]<br>\n",
    "db is <br>\n",
    " [[0.37345476]<br>\n",
    " [0.34414727]<br>\n",
    " [0.29074635]]<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L_layer_backward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graded\n",
    "\n",
    "def L_layer_backward(HL, Y, memories):\n",
    "    \n",
    "    # Takes the predicted value HL and the true target value Y and the \n",
    "    # memories calculated by L_layer_forward as input\n",
    "    \n",
    "    # returns the gradients calulated for all the layers as a dict\n",
    "    \n",
    "    gradients = {}\n",
    "    L = len(memories) # the number of layers\n",
    "    m = HL.shape[1]\n",
    "    Y = Y.reshape(HL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Perform the backprop for the last layer that is the softmax layer\n",
    "    current_memory = memories[-1]\n",
    "    linear_memory, activation_memory = current_memory\n",
    "    dZ = HL - Y\n",
    "    H_prev, W, b = linear_memory\n",
    "    \n",
    "    # Use the expressions you have used in 'layer_backward'\n",
    "    gradients[\"dH\" + str(L-1)] = np.dot(np.transpose(W),dZ)#write your code here\n",
    "    gradients[\"dW\" + str(L)] = (1.0/m) * np.matmul(dZ, H_prev.T) #write your code here, use (1./m) and not (1/m)\n",
    "    gradients[\"db\" + str(L)] = (1.0/m) * np.sum(dZ, axis=-1, keepdims=True) #write your code here, use (1./m) and not (1/m)\n",
    "    \n",
    "    # Perform the backpropagation l-1 times\n",
    "    for l in reversed(range(L-1)):\n",
    "        # Lth layer gradients: \"gradients[\"dH\" + str(l + 1)] \", gradients[\"dW\" + str(l + 2)] , gradients[\"db\" + str(l + 2)]\n",
    "        current_memory = memories[l]\n",
    "        \n",
    "        dH_prev_temp, dW_temp, db_temp = layer_backward(gradients[\"dH\"+ str(l+1)] ,current_memory,activation=\"relu\")\n",
    "        gradients[\"dH\" + str(l)] = dH_prev_temp\n",
    "        gradients[\"dW\" + str(l + 1)] = dW_temp\n",
    "        gradients[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW3 is \n",
      " [[ 0.02003701  0.0019043   0.01011729  0.0145757   0.00146444  0.00059863\n",
      "   0.        ]\n",
      " [ 0.02154547  0.00203519  0.01085648  0.01567075  0.00156469  0.00060533\n",
      "   0.        ]\n",
      " [-0.01718407 -0.00273711 -0.00499101 -0.00912135 -0.00207365  0.00059996\n",
      "   0.        ]\n",
      " [-0.01141498 -0.00158622 -0.00607049 -0.00924709 -0.00119619  0.00060381\n",
      "   0.        ]\n",
      " [ 0.01943173  0.0018421   0.00984543  0.01416368  0.00141676  0.00059682\n",
      "   0.        ]\n",
      " [ 0.01045447  0.00063974  0.00637621  0.00863306  0.00050118  0.00060441\n",
      "   0.        ]\n",
      " [-0.06338911 -0.00747251 -0.0242169  -0.03835708 -0.00581131  0.0006034\n",
      "   0.        ]\n",
      " [ 0.01911373  0.001805    0.00703101  0.0120636   0.00138836 -0.00140535\n",
      "   0.        ]\n",
      " [-0.01801603  0.0017357  -0.01489228 -0.02026076  0.00133528  0.00060264\n",
      "   0.        ]\n",
      " [ 0.0194218   0.00183381  0.00594427  0.01187949  0.00141043 -0.00340965\n",
      "   0.        ]]\n",
      "db3 is \n",
      " [[ 0.10031756]\n",
      " [ 0.00460183]\n",
      " [-0.00142942]\n",
      " [-0.0997827 ]\n",
      " [ 0.09872663]\n",
      " [ 0.00536378]\n",
      " [-0.10124784]\n",
      " [-0.00191121]\n",
      " [-0.00359044]\n",
      " [-0.00104818]]\n",
      "dW2 is \n",
      " [[  4.94428956e-05   1.13215514e-02   5.44180380e-02]\n",
      " [ -4.81267081e-05  -2.96999448e-05  -1.81899582e-02]\n",
      " [  5.63424333e-05   4.77190073e-03   4.04810232e-02]\n",
      " [  1.49767478e-04  -1.89780927e-03  -7.91231369e-03]\n",
      " [  1.97866094e-04   1.22107085e-04   2.64140566e-02]\n",
      " [  0.00000000e+00  -3.75805770e-04   1.63906102e-05]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00]]\n",
      "db2 is \n",
      " [[ 0.013979  ]\n",
      " [-0.01329383]\n",
      " [ 0.01275707]\n",
      " [-0.01052957]\n",
      " [ 0.03179224]\n",
      " [-0.00039877]\n",
      " [ 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# verify\n",
    "# X is (784, 10)\n",
    "# parameters is a dict\n",
    "# HL should be (10, 10)\n",
    "x_sample = train_set_x[:, 10:20]\n",
    "y_sample = train_set_y[:, 10:20]\n",
    "\n",
    "HL, memories = L_layer_forward(x_sample, parameters=parameters)\n",
    "gradients  = L_layer_backward(HL, y_sample, memories)\n",
    "print('dW3 is \\n', gradients['dW3'])\n",
    "print('db3 is \\n', gradients['db3'])\n",
    "print('dW2 is \\n', gradients['dW2'])\n",
    "print('db2 is \\n', gradients['db2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get:<br>\n",
    "\n",
    "dW3 is <br>\n",
    " [[ 0.02003701  0.0019043   0.01011729  0.0145757   0.00146444  0.00059863  0.        ]<br>\n",
    " [ 0.02154547  0.00203519  0.01085648  0.01567075  0.00156469  0.00060533   0.        ]<br>\n",
    " [-0.01718407 -0.00273711 -0.00499101 -0.00912135 -0.00207365  0.00059996   0.        ]<br>\n",
    " [-0.01141498 -0.00158622 -0.00607049 -0.00924709 -0.00119619  0.00060381   0.        ]<br>\n",
    " [ 0.01943173  0.0018421   0.00984543  0.01416368  0.00141676  0.00059682   0.        ]<br>\n",
    " [ 0.01045447  0.00063974  0.00637621  0.00863306  0.00050118  0.00060441   0.        ]<br>\n",
    " [-0.06338911 -0.00747251 -0.0242169  -0.03835708 -0.00581131  0.0006034   0.        ]<br>\n",
    " [ 0.01911373  0.001805    0.00703101  0.0120636   0.00138836 -0.00140535   0.        ]<br>\n",
    " [-0.01801603  0.0017357  -0.01489228 -0.02026076  0.00133528  0.00060264   0.        ]<br>\n",
    " [ 0.0194218   0.00183381  0.00594427  0.01187949  0.00141043 -0.00340965    0.        ]]<br>\n",
    "db3 is <br>\n",
    " [[ 0.10031756]<br>\n",
    " [ 0.00460183]<br>\n",
    " [-0.00142942]<br>\n",
    " [-0.0997827 ]<br>\n",
    " [ 0.09872663]<br>\n",
    " [ 0.00536378]<br>\n",
    " [-0.10124784]<br>\n",
    " [-0.00191121]<br>\n",
    " [-0.00359044]<br>\n",
    " [-0.00104818]]<br>\n",
    "dW2 is <br>\n",
    " [[ 4.94428956e-05  1.13215514e-02  5.44180380e-02]<br>\n",
    " [-4.81267081e-05 -2.96999448e-05 -1.81899582e-02]<br>\n",
    " [ 5.63424333e-05  4.77190073e-03  4.04810232e-02]<br>\n",
    " [ 1.49767478e-04 -1.89780927e-03 -7.91231369e-03]<br>\n",
    " [ 1.97866094e-04  1.22107085e-04  2.64140566e-02]<br>\n",
    " [ 0.00000000e+00 -3.75805770e-04  1.63906102e-05]<br>\n",
    " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]]<br>\n",
    "db2 is <br>\n",
    " [[ 0.013979  ]<br>\n",
    " [-0.01329383]<br>\n",
    " [ 0.01275707]<br>\n",
    " [-0.01052957]<br>\n",
    " [ 0.03179224]<br>\n",
    " [-0.00039877]<br>\n",
    " [ 0.        ]]<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Updates\n",
    "\n",
    "Now that we have calculated the gradients. let's do the last step which is updating the weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graded\n",
    "\n",
    "def update_parameters(parameters, gradients, learning_rate):\n",
    "\n",
    "    # parameters is the python dictionary containing the parameters W and b for all the layers\n",
    "    # gradients is the python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    # returns updated weights after applying the gradient descent update\n",
    "\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*gradients[\"dW\" + str(l + 1)] \n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*gradients[\"db\" + str(l + 1)]\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having defined the bits and pieces of the feedforward and the backpropagation, let's now combine all that to form a model. The list `dimensions` has the number of neurons in each layer specified in it. For a neural network with 1 hidden layer with 45 neurons, you would specify the dimensions as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = [784, 45, 10] #  three-layer model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "### L_layer_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graded\n",
    "\n",
    "def L_layer_model(X, Y, dimensions, learning_rate = 0.0075, num_iterations = 3000, print_loss=False):\n",
    "    \n",
    "    # X and Y are the input training datasets\n",
    "    # learning_rate, num_iterations are gradient descent optimization parameters\n",
    "    # returns updated parameters\n",
    "\n",
    "    np.random.seed(2)\n",
    "    losses = []                         # keep track of loss\n",
    "    \n",
    "    # Parameters initialization\n",
    "    parameters = initialize_parameters(dimensions)\n",
    " \n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation\n",
    "        HL, memories =  L_layer_forward(X, parameters)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = compute_loss(HL, Y)\n",
    "    \n",
    "        # Backward propagation\n",
    "        gradients = L_layer_backward(HL,Y, memories)\n",
    " \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "                \n",
    "        # Printing the loss every 100 training example\n",
    "        if print_loss and i % 100 == 0:\n",
    "            print (\"Loss after iteration %i: %f\" %(i, loss))\n",
    "            losses.append(loss)\n",
    "            \n",
    "    # plotting the loss\n",
    "    plt.plot(np.squeeze(losses))\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, it'll take a lot of time to train the model on 50,000 data points, we take a subset of 5,000 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 5000)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_x_new = train_set_x[:,0:5000]\n",
    "train_set_y_new = train_set_y[:,0:5000]\n",
    "train_set_x_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's call the function L_layer_model on the dataset we have created.This will take 10-20 mins to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 2.422624\n",
      "Loss after iteration 100: 2.129232\n",
      "Loss after iteration 200: 1.876095\n",
      "Loss after iteration 300: 1.604213\n",
      "Loss after iteration 400: 1.350205\n",
      "Loss after iteration 500: 1.144823\n",
      "Loss after iteration 600: 0.990554\n",
      "Loss after iteration 700: 0.876603\n",
      "Loss after iteration 800: 0.791154\n",
      "Loss after iteration 900: 0.725441\n",
      "Loss after iteration 1000: 0.673485\n",
      "Loss after iteration 1100: 0.631386\n",
      "Loss after iteration 1200: 0.596598\n",
      "Loss after iteration 1300: 0.567342\n",
      "Loss after iteration 1400: 0.542346\n",
      "Loss after iteration 1500: 0.520746\n",
      "Loss after iteration 1600: 0.501865\n",
      "Loss after iteration 1700: 0.485205\n",
      "Loss after iteration 1800: 0.470368\n",
      "Loss after iteration 1900: 0.457054\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7faf996f1c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = L_layer_model(train_set_x_new, train_set_y_new, dimensions, num_iterations = 2000, print_loss = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters):\n",
    "    \n",
    "    # Performs forward propogation using the trained parameters and calculates the accuracy\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = L_layer_forward(X, parameters)\n",
    "    \n",
    "    p = np.argmax(probas, axis = 0)\n",
    "    act = np.argmax(y, axis = 0)\n",
    "\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == act)/m)))\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the accuray we get on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8774\n"
     ]
    }
   ],
   "source": [
    "pred_train = predict(train_set_x_new, train_set_y_new, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get ~ 88% accuracy on the training data. Let's see the accuray on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8674\n"
     ]
    }
   ],
   "source": [
    "pred_test = predict(test_set_x, test_set_y, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is ~87%. You can train the model even longer and get better result. You can also try to change the network structure. \n",
    "<br>Below, you can see which all numbers are incorrectly identified by the neural network by changing the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7faf9145c668>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7faf9145c470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index  = 3474\n",
    "k = test_set_x[:,index]\n",
    "k = k.reshape((28, 28))\n",
    "plt.title('Label is {label}'.format(label=(pred_test[index], np.argmax(test_set_y, axis = 0)[index])))\n",
    "plt.imshow(k, cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
